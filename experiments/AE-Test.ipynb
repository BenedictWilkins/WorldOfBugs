{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4b27b9a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "#%pprint\n",
    "import notebook\n",
    "import os\n",
    "import torchsummary\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, ConcatDataset, DataLoader\n",
    "from types import SimpleNamespace\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "import sklearn\n",
    "import sklearn.metrics\n",
    "\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import utils\n",
    "from SSIM import SSIM # structural similarity loss...\n",
    "\n",
    "import jnu as J\n",
    "\n",
    "config = dict(\n",
    "    device = \"cuda:0\",\n",
    "    state_shape = (3,84,84),\n",
    "    latent_shape = (256,),\n",
    ")\n",
    "config = SimpleNamespace(**config)\n",
    "\n",
    "model = utils.AE(config.state_shape, config.latent_shape)\n",
    "model.load_state_dict(torch.load(\"./AE-SSIM-256-3.pt\"))\n",
    "model = model.to(config.device)\n",
    "#criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "criterion = SSIM(size_average=False)\n",
    "\n",
    "label_thresholds = list(reversed([0, 10, 50, 100, 200, 400]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc20c22",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(5,4, sharex=True, sharey=True, figsize=(10,10), \n",
    "                         gridspec_kw=dict(wspace=-0.2, hspace=0.2, left=0.05, right=.95, top=.95, bottom=0.05))\n",
    "import matplotlib as mpl\n",
    "cmap = mpl.colormaps['viridis']\n",
    "\n",
    "\n",
    "#fig.tight_layout()\n",
    "axes = axes.ravel()\n",
    "for ax in axes:\n",
    "    ax.set_aspect('equal')\n",
    "    ax.set_prop_cycle('color',cmap(1 - np.linspace(0,1,len(label_thresholds))))\n",
    "    \n",
    "font = {'fontsize':10}\n",
    "plt.set_cmap(\"viridis\")\n",
    "\n",
    "metrics = dict(auc_pr={\"Precision/Recall AUC\":[], \"τ\":label_thresholds}, \n",
    "               auc_roc={\"ROC AUC\":[], \"τ\":label_thresholds})\n",
    "\n",
    "for i, file in enumerate(utils.FILES_TEST):\n",
    "    name = os.path.splitext(os.path.split(file)[1])[0].replace(\"Geometry\", \"Geom\").replace(\"Texture\", \"Tex\")\n",
    "    print(name)\n",
    "    ys, ys_ = [], []\n",
    "    i = 2 * i\n",
    "    for x, y in utils.load(file, ['observation', 'bugmask']):\n",
    "        x = torch.from_numpy(x).to(config.device)\n",
    "        x_ = utils.forward(model, x, device=config.device)\n",
    "        ys_.append(criterion(x, x_).cpu().numpy()) # score (1D)\n",
    "        y = (y[...].sum(1, keepdims=True) > 0.)\n",
    "        ys.append(y.sum(-1).sum(-1).sum(-1)) # label (1D)\n",
    "    y, y_ = np.concatenate(ys), np.concatenate(ys_)\n",
    "    \n",
    "    # compute metrics, ROC\n",
    "    plt.set_cmap(\"viridis\")\n",
    "    thresholds, fprtpr, aucs = utils.roc(y, y_, label_thresholds)\n",
    "    for lt, (fpr, tpr), auc in zip(thresholds, fprtpr, aucs):  \n",
    "        axes[i].plot(fpr, tpr, alpha=0.8, label=f\"τ={lt}\")    \n",
    "    axes[i].set_title(name + \" (ROC)\", fontdict=font)\n",
    "    metrics['auc_roc'][name] = aucs\n",
    "    \n",
    "    # Precision recall\n",
    "    thresholds, rp, aucs = utils.pr(y, y_, label_thresholds)\n",
    "    for lt, (r, p), auc in zip(thresholds, rp, aucs):  \n",
    "        axes[i + 1].plot(r, p, alpha=0.8, label=f\"τ={lt}\")\n",
    "    axes[i + 1].set_title(name + \" (PR)\", fontdict=font)\n",
    "    metrics['auc_pr'][name] = aucs\n",
    "    \n",
    "axes[-1].legend()\n",
    "\n",
    "def format_x(x):\n",
    "    if isinstance(x, int):\n",
    "        return str(x)\n",
    "    elif isinstance(x, float):\n",
    "        return \"{0:0.3f}\".format(x)\n",
    "    raise ValueError()\n",
    "\n",
    "print(\"Auto-Encoder (SSIM)\")\n",
    "for k,v in metrics.items():\n",
    "    for n, x in v.items():\n",
    "        print(\" & \".join([n] + list(reversed([format_x(i) for i in x]))) + \" \\\\\\\\\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "604c39b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.savefig(\"./AE-SIM-256-ROC-PR.png\", dpi=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "086b11bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPLORE SINGLE\n",
    "file_id = 6 #2 # 5\n",
    "file = utils.FILES_TEST[file_id]\n",
    "\n",
    "name = os.path.splitext(os.path.split(file)[1])[0]\n",
    "print(name)\n",
    "ys, ys_ = [], []\n",
    "i = 2 * i\n",
    "for x, y in utils.load(file, ['observation', 'bugmask']):\n",
    "    x = torch.from_numpy(x).to(config.device)\n",
    "    x_ = utils.forward(model, x, device=config.device)\n",
    "    \n",
    "    J.images(np.concatenate([x.cpu().numpy(), x_.cpu().numpy(), y], axis=3), scale=3)\n",
    "        \n",
    "    \n",
    "    ys_.append(criterion(x, x_).cpu().numpy()) # score (1D)\n",
    "    y = (y[...].sum(1, keepdims=True) > 0.)\n",
    "    ys.append(y.sum(-1).sum(-1).sum(-1)) # label (1D)\n",
    "    break\n",
    "    \n",
    "y, y_ = np.concatenate(ys), np.concatenate(ys_)\n",
    "y = (y > 0).astype(np.float32)\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "plt.plot(np.arange(y.shape[0]), y, alpha=0.8)\n",
    "plt.plot(np.arange(y_.shape[0]), y_, alpha=0.8)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a92b1bfd",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def model_forward(model, obs):\n",
    "    with torch.no_grad():\n",
    "        loader = DataLoader(obs, batch_size=512)\n",
    "        result = [model(x) for x in loader]\n",
    "        return torch.clip(torch.cat(result),0,1)\n",
    "\n",
    "\n",
    "    \n",
    "def metrics(label, score, thresholds=np.arange(1,20) * 0.05, normalize=None, beta=1.0):\n",
    "    assert label.shape[0] == score.shape[0]\n",
    "    assert len(label.shape) == len(score.shape)\n",
    "    score = np.interp(score, (score.min(), score.max()), (0, 1)).astype(np.float32)\n",
    "    #label = np.interp(label, (label.min(), label.max()), (0, 1))\n",
    "    label = (label > 0).astype(np.float32)\n",
    "    \n",
    "    prf = []\n",
    "    cms = []\n",
    "    \n",
    "    \n",
    "    for threshold in thresholds:\n",
    "        _score = (score > threshold).astype(np.float32)\n",
    "        cms.append(sklearn.metrics.confusion_matrix(label, _score, normalize=normalize).ravel())\n",
    "        prf.append(sklearn.metrics.precision_recall_fscore_support(label, _score, beta=beta, pos_label=1, average='binary'))\n",
    "        print(threshold, prf[-1])\n",
    "        \n",
    "        #plt.figure()\n",
    "        #plt.plot(label, alpha=0.6, label=\"label\")\n",
    "        #plt.plot(_score, alpha=0.6, label=\"score\")\n",
    "        #plt.legend()\n",
    "    cms = np.stack(cms)\n",
    "    fig = plt.figure()\n",
    "    for i, l in enumerate([\"TN\", \"FP\", \"FN\", \"TP\"]):\n",
    "        plt.plot(thresholds, cms[:,i], label=l)\n",
    "    plt.legend()\n",
    "    \n",
    "    #tn, fp, fn, tp\n",
    "    \n",
    "def roc(label, score, label_thresholds=[0, 10, 50, 100, 200], alpha=0.5, ax=None):\n",
    "    assert label.shape[0] == score.shape[0]\n",
    "    assert len(label.shape) == len(score.shape)\n",
    "    #score = np.interp(score, (score.min(), score.max()), (0, 1)).astype(np.float32)\n",
    "    results = []\n",
    "    print(label.min(), label.max())\n",
    "    for lt in label_thresholds:\n",
    "        _label = (label > lt).astype(np.float32)\n",
    "        fpr, tpr, thresholds = sklearn.metrics.roc_curve(_label, score) \n",
    "        auc = sklearn.metrics.auc(fpr, tpr)\n",
    "        results.append((fpr, tpr, thresholds))\n",
    "        if ax is not None:\n",
    "            ax.plot(fpr, tpr, alpha=alpha, label=f\"auc={auc:.3f}, τ={lt}\")\n",
    "            ax.set_xlabel(\"false positive rate\")\n",
    "            ax.set_ylabel(\"true positive rate\")\n",
    "    if ax is not None:\n",
    "        ax.legend()\n",
    "    return results, label_thresholds\n",
    "\n",
    "\n",
    "def pr(label, score, label_thresholds=[0, 10, 50, 100, 200], alpha=0.5, ax=None):\n",
    "    assert label.shape[0] == score.shape[0]\n",
    "    assert len(label.shape) == len(score.shape)\n",
    "    #score = np.interp(score, (score.min(), score.max()), (0, 1)).astype(np.float32)\n",
    "    results = []\n",
    "    print(label.min(), label.max())\n",
    "    for lt in label_thresholds:\n",
    "        _label = (label > lt).astype(np.float32)\n",
    "        p, r, thresholds = sklearn.metrics.precision_recall_curve(_label, score) \n",
    "        auc = sklearn.metrics.auc(r, p)\n",
    "        results.append((p, r, thresholds))\n",
    "        if ax is not None:\n",
    "            ax.plot(r, p, alpha=alpha, label=f\"auc={auc:.3f}, τ={lt}\")\n",
    "            ax.set_xlabel(\"Recall\")\n",
    "            ax.set_ylabel(\"Precision\")\n",
    "    if ax is not None:\n",
    "        ax.legend()\n",
    "    return results, label_thresholds\n",
    "    \n",
    "select = [0]\n",
    "plot = True\n",
    "LABEL_THRESHOLDS = [0,10,50,100,200,400]\n",
    "\n",
    "for bug, file in FILES:\n",
    "    print(bug)\n",
    "    for i, (obs, label) in enumerate(load(file=file, keys=[\"observation\", \"bugmask\"])):\n",
    "        if not i in select:\n",
    "            continue\n",
    "        #obs, label = obs[:100], label[:100]\n",
    "        obs, label = obs[...], label[...]\n",
    "        obs = torch.from_numpy(obs[...]).to(config.device)\n",
    "        pred = model_forward(model, obs)\n",
    "        label = (label[...].sum(1, keepdims=True) > 0.)\n",
    "        \n",
    "        J.images(np.concatenate([obs.cpu().numpy(), pred.cpu().numpy(), label.repeat(3, axis=1)], axis=3), scale=3)\n",
    "        \n",
    "        score = criterion(pred, obs).cpu().numpy()\n",
    "        label = label.sum(-1).sum(-1).sum(-1)  \n",
    "        assert label.sum() > 0 # there must be some bugs present in the episode!\n",
    "        \n",
    "            \n",
    "        if plot:\n",
    "            _score = np.interp(score, (score.min(), score.max()), (0, 1)).astype(np.float32)\n",
    "            _label = np.interp(label, (label.min(), label.max()), (0, 1)).astype(np.float32)\n",
    "            fig = plt.figure(figsize=(10,3))\n",
    "            plt.plot(np.arange(_score.shape[0]), _score, label=\"score\")\n",
    "            plt.plot(np.arange(_label.shape[0]), _label, label=\"label\", alpha=0.5)\n",
    "            plt.legend()\n",
    "            \n",
    "        #metrics(label, score)\n",
    "        fig, _ = plt.subplots(1, 2, figsize=(8, 4))\n",
    "        plt.tight_layout(pad=2)\n",
    "        roc(label, score, ax=fig.axes[0], label_thresholds=LABEL_THRESHOLDS)\n",
    "        pr(label, score, ax=fig.axes[1], label_thresholds=LABEL_THRESHOLDS)\n",
    "        #\n",
    "            \n",
    "        \n",
    "        break\n",
    "        \n",
    "    print(\"----------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73c2ea3f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe6e0d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dict()\n",
    "for (obs, label) in test_data:\n",
    "    print(np.unique(label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da1c3a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AE(config.state_shape, config.latent_shape)\n",
    "model.load_state_dict(torch.load(\"./AE-SSIM-256.pt\"))\n",
    "#criterion = nn.BCEWithLogitsLoss(reduction='none')\n",
    "criterion = SSIM(size_average=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb5e69b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    n = 1024\n",
    "    model = model.cpu()\n",
    "    for obs, mask in test_data:\n",
    "        obs, mask = obs[:n], mask[:n]\n",
    "        obs = obs.cpu()\n",
    "        pred = model(obs).cpu()\n",
    "        score = criterion(pred, obs)\n",
    "        print(score.shape)\n",
    "        \n",
    "        score = score.reshape(score.shape[0],-1).sum(-1).cpu().numpy()\n",
    "        score = np.interp(score, (score.min(), score.max()), (0, +1))\n",
    "        \n",
    "        label = mask.reshape(mask.shape[0],-1).sum(-1).cpu().numpy()\n",
    "        label = np.interp(label, (label.min(), label.max()), (0, +1))\n",
    "        \n",
    "        J.images(torch.cat([torch.clip(pred,0,1), obs.cpu(), mask], dim=3))\n",
    "        fig = plt.figure(figsize=(10,5))\n",
    "        plt.plot(np.arange(score.shape[0]), score, label=\"score\")\n",
    "        plt.plot(np.arange(score.shape[0]), label, label=\"label\")\n",
    "        plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3865719",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PhD",
   "language": "python",
   "name": "phd"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
